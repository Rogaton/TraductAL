{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune NLLB-200-distilled-600M on Google Colab\n",
        "\n",
        "This notebook fine-tunes NLLB-200 on the opus-100 dataset with:\n",
        "- ✅ **Automatic checkpoint saving to HuggingFace Hub** (no data loss)\n",
        "- ✅ **Resume from checkpoint** if interrupted\n",
        "- ✅ **Optimized for T4 GPU** (free on Colab)\n",
        "\n",
        "**Hardware**: Runtime → Change runtime type → T4 GPU\n",
        "\n",
        "**Time**: ~2-3 hours for 3 language pairs"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup and Installation"
      ],
      "metadata": {
        "id": "setup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers>=4.35.0 datasets>=2.14.0 accelerate>=0.24.0 sentencepiece protobuf tensorboard"
      ],
      "metadata": {
        "id": "install"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "check-gpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. HuggingFace Login\n",
        "\n",
        "Get your token from: https://huggingface.co/settings/tokens"
      ],
      "metadata": {
        "id": "login"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Enter your HuggingFace token\n",
        "HF_TOKEN = \"\"  # Paste your token here\n",
        "\n",
        "login(token=HF_TOKEN)"
      ],
      "metadata": {
        "id": "hf-login"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Configuration\n",
        "\n",
        "Customize your training parameters here:"
      ],
      "metadata": {
        "id": "config"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training configuration\n",
        "CONFIG = {\n",
        "    \"model_name\": \"facebook/nllb-200-distilled-600M\",\n",
        "    \"language_pairs\": [\"en-fr\", \"en-de\", \"en-es\"],  # Customize!\n",
        "    \"max_samples_per_pair\": 50000,\n",
        "    \"epochs\": 3,\n",
        "    \"batch_size\": 4,\n",
        "    \"learning_rate\": 5e-5,\n",
        "    \n",
        "    # HuggingFace Hub settings\n",
        "    \"hf_username\": \"YOUR_USERNAME\",  # CHANGE THIS!\n",
        "    \"hub_repo_name\": \"fine-tuned-nllb-600M\",\n",
        "    \n",
        "    # Resume from checkpoint (leave None to start fresh)\n",
        "    \"resume_from_checkpoint\": None,  # Or \"YOUR_USERNAME/fine-tuned-nllb-600M\"\n",
        "}\n",
        "\n",
        "# Validate configuration\n",
        "assert CONFIG[\"hf_username\"] != \"YOUR_USERNAME\", \"Please set your HuggingFace username!\"\n",
        "\n",
        "print(\"✅ Configuration loaded\")\n",
        "print(f\"Training pairs: {CONFIG['language_pairs']}\")\n",
        "print(f\"Hub repo: {CONFIG['hf_username']}/{CONFIG['hub_repo_name']}\")"
      ],
      "metadata": {
        "id": "config-vars"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Download Training Script"
      ],
      "metadata": {
        "id": "download-script"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Option A: Upload train_nllb_hf_spaces.py manually via Files panel\n",
        "# Option B: Download from your repository\n",
        "\n",
        "# For now, we'll create it inline\n",
        "!wget https://raw.githubusercontent.com/YOUR_REPO/train_nllb_hf_spaces.py -O train_nllb_hf_spaces.py\n",
        "\n",
        "# Or upload manually: Click folder icon → Upload → Select train_nllb_hf_spaces.py"
      ],
      "metadata": {
        "id": "download"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Start Training\n",
        "\n",
        "**Important**: Checkpoints are automatically saved to HuggingFace Hub every 500 steps!\n",
        "\n",
        "Monitor your Hub repo: https://huggingface.co/YOUR_USERNAME/fine-tuned-nllb-600M"
      ],
      "metadata": {
        "id": "train"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build command\n",
        "hub_model_id = f\"{CONFIG['hf_username']}/{CONFIG['hub_repo_name']}\"\n",
        "\n",
        "cmd = [\n",
        "    \"python\", \"train_nllb_hf_spaces.py\",\n",
        "    \"--language_pairs\", *CONFIG[\"language_pairs\"],\n",
        "    \"--max_samples\", str(CONFIG[\"max_samples_per_pair\"]),\n",
        "    \"--epochs\", str(CONFIG[\"epochs\"]),\n",
        "    \"--batch_size\", str(CONFIG[\"batch_size\"]),\n",
        "    \"--learning_rate\", str(CONFIG[\"learning_rate\"]),\n",
        "    \"--push_to_hub\",\n",
        "    \"--hub_repo_name\", CONFIG[\"hub_repo_name\"],\n",
        "    \"--hf_username\", CONFIG[\"hf_username\"],\n",
        "]\n",
        "\n",
        "# Add resume checkpoint if specified\n",
        "if CONFIG[\"resume_from_checkpoint\"]:\n",
        "    cmd.extend([\"--resume_from_checkpoint\", CONFIG[\"resume_from_checkpoint\"]])\n",
        "\n",
        "print(\"Starting training with command:\")\n",
        "print(\" \".join(cmd))\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Run training\n",
        "import subprocess\n",
        "subprocess.run(cmd)"
      ],
      "metadata": {
        "id": "run-training"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Monitor Training\n",
        "\n",
        "Check your HuggingFace Hub repository to see checkpoints:\n",
        "\n",
        "https://huggingface.co/YOUR_USERNAME/fine-tuned-nllb-600M\n",
        "\n",
        "You should see:\n",
        "- `checkpoint-500/`\n",
        "- `checkpoint-1000/`\n",
        "- `checkpoint-1500/`\n",
        "- etc.\n",
        "\n",
        "**If Colab disconnects**: Simply restart this notebook and set:\n",
        "```python\n",
        "CONFIG[\"resume_from_checkpoint\"] = \"YOUR_USERNAME/fine-tuned-nllb-600M\"\n",
        "```\n",
        "\n",
        "Then re-run the training cell!"
      ],
      "metadata": {
        "id": "monitor"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Test Fine-tuned Model"
      ],
      "metadata": {
        "id": "test"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "# Load your fine-tuned model from Hub\n",
        "model_name = f\"{CONFIG['hf_username']}/{CONFIG['hub_repo_name']}\"\n",
        "\n",
        "print(f\"Loading model: {model_name}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Test translation (English to French)\n",
        "test_text = \"Hello, how are you today?\"\n",
        "print(f\"\\nTest input: '{test_text}'\")\n",
        "\n",
        "tokenizer.src_lang = \"eng_Latn\"\n",
        "inputs = tokenizer(test_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "translated_tokens = model.generate(\n",
        "    **inputs,\n",
        "    forced_bos_token_id=tokenizer.lang_code_to_id[\"fra_Latn\"],\n",
        "    max_length=512,\n",
        "    num_beams=5\n",
        ")\n",
        "\n",
        "translation = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
        "print(f\"Test output (French): '{translation}'\")\n",
        "print(\"\\n✅ Model test successful!\")"
      ],
      "metadata": {
        "id": "test-model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Download to Local Machine\n",
        "\n",
        "On your local machine, run:\n",
        "\n",
        "```bash\n",
        "python download_finetuned_model.py \\\n",
        "    --model_name YOUR_USERNAME/fine-tuned-nllb-600M \\\n",
        "    --test\n",
        "```"
      ],
      "metadata": {
        "id": "download-local"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tips for Success\n",
        "\n",
        "### Preventing Colab Disconnects\n",
        "\n",
        "1. **Keep browser tab active** - Colab may disconnect if tab is inactive\n",
        "2. **Use Colab Pro** - Longer timeout periods ($10/month)\n",
        "3. **Enable background execution** - Colab Pro feature\n",
        "4. **Don't worry about disconnects** - Checkpoints are on Hub!\n",
        "\n",
        "### If Training Stops\n",
        "\n",
        "1. Check your Hub repo for last checkpoint\n",
        "2. Set `CONFIG[\"resume_from_checkpoint\"]` to your Hub repo\n",
        "3. Re-run training cell\n",
        "4. Training continues from last checkpoint!\n",
        "\n",
        "### Monitoring Progress\n",
        "\n",
        "- **Hub repo**: https://huggingface.co/YOUR_USERNAME/fine-tuned-nllb-600M\n",
        "- **TensorBoard**: Run in another cell: `%load_ext tensorboard; %tensorboard --logdir ./fine-tuned-nllb/logs`\n",
        "\n",
        "### Estimated Timeline\n",
        "\n",
        "- First checkpoint (~500 steps): 10-15 minutes\n",
        "- Total training (3 language pairs, 50k each): 2-3 hours\n",
        "- Checkpoints every 500 steps: ~10-15 checkpoints total"
      ],
      "metadata": {
        "id": "tips"
      }
    }
  ]
}
